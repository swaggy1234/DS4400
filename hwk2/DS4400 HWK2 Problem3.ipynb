{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52a9cbb1-bcac-4fdd-bf09-7ce5af6ea424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Closed-form (my implementation) ===\n",
      "Training MSE: 31043433137.295128\n",
      "Training R^2: 0.7303787115834163\n",
      "Testing  MSE: 58389284496.827225\n",
      "Testing  R^2: 0.6497909600248234\n",
      "\n",
      "=== sklearn LinearRegression (Problem 2 package) ===\n",
      "Training MSE: 31043433137.295223\n",
      "Training R^2: 0.7303787115834155\n",
      "Testing  MSE: 58389284496.820786\n",
      "Testing  R^2: 0.649790960024862\n",
      "\n",
      "Max abs coef difference: 3.511621559937339e-05\n",
      "Abs intercept difference: 4.008971154689789e-05\n",
      "\n",
      "Example predict_one on first test sample: 704990.0797025487\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# =========================\n",
    "# 1) Load data (same files as Problem 2)\n",
    "# =========================\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df  = pd.read_csv(\"test.csv\")\n",
    "\n",
    "target = \"price\"\n",
    "\n",
    "# =========================\n",
    "# 2) Features/target + one-hot encode + align\n",
    "# =========================\n",
    "X_train = pd.get_dummies(train_df.drop(columns=[target]), drop_first=True)\n",
    "y_train = train_df[target].to_numpy()\n",
    "\n",
    "X_test  = pd.get_dummies(test_df.drop(columns=[target]), drop_first=True)\n",
    "y_test  = test_df[target].to_numpy()\n",
    "\n",
    "# Make sure train/test have the same columns (very important)\n",
    "X_train, X_test = X_train.align(X_test, join=\"left\", axis=1, fill_value=0)\n",
    "\n",
    "# Convert to numpy\n",
    "Xtr = X_train.to_numpy()\n",
    "Xte = X_test.to_numpy()\n",
    "\n",
    "# =========================\n",
    "# 3) Closed-form (Normal Equation) from class\n",
    "#    theta = (X^T X)^(-1) X^T y\n",
    "#    where X includes a column of 1s for intercept\n",
    "# =========================\n",
    "Xtr_b = np.c_[np.ones((Xtr.shape[0], 1)), Xtr]   # add bias column\n",
    "Xte_b = np.c_[np.ones((Xte.shape[0], 1)), Xte]\n",
    "\n",
    "# Use the closed-form least squares solution (equivalent to normal equation)\n",
    "# lstsq solves: min ||Xtr_b * theta - y||^2\n",
    "theta = np.linalg.lstsq(Xtr_b, y_train, rcond=None)[0]\n",
    "\n",
    "# =========================\n",
    "# 4) Prediction functions \n",
    "# =========================\n",
    "def predict_one(x_row, theta):\n",
    "    \"\"\"\n",
    "    Predict response for ONE new testing point.\n",
    "    x_row: numpy array shape (d,) in SAME feature order as X_train.columns\n",
    "    \"\"\"\n",
    "    x_row_b = np.r_[1.0, x_row]     # prepend 1 for intercept\n",
    "    return float(x_row_b @ theta)\n",
    "\n",
    "def predict_batch(X, theta):\n",
    "    \"\"\"\n",
    "    Predict responses for MANY points.\n",
    "    X: numpy array shape (N, d)\n",
    "    \"\"\"\n",
    "    X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "    return X_b @ theta\n",
    "\n",
    "# Closed-form predictions\n",
    "y_train_pred_cf = predict_batch(Xtr, theta)\n",
    "y_test_pred_cf  = predict_batch(Xte, theta)\n",
    "\n",
    "# Closed-form metrics\n",
    "cf_train_mse = mean_squared_error(y_train, y_train_pred_cf)\n",
    "cf_train_r2  = r2_score(y_train, y_train_pred_cf)\n",
    "cf_test_mse  = mean_squared_error(y_test, y_test_pred_cf)\n",
    "cf_test_r2   = r2_score(y_test, y_test_pred_cf)\n",
    "\n",
    "print(\"=== Closed-form (my implementation) ===\")\n",
    "print(\"Training MSE:\", cf_train_mse)\n",
    "print(\"Training R^2:\", cf_train_r2)\n",
    "print(\"Testing  MSE:\", cf_test_mse)\n",
    "print(\"Testing  R^2:\", cf_test_r2)\n",
    "\n",
    "# =========================\n",
    "# 5) Compare with package (Problem 2 sklearn LinearRegression)\n",
    "# =========================\n",
    "sk = LinearRegression()\n",
    "sk.fit(Xtr, y_train)\n",
    "\n",
    "y_train_pred_sk = sk.predict(Xtr)\n",
    "y_test_pred_sk  = sk.predict(Xte)\n",
    "\n",
    "sk_train_mse = mean_squared_error(y_train, y_train_pred_sk)\n",
    "sk_train_r2  = r2_score(y_train, y_train_pred_sk)\n",
    "sk_test_mse  = mean_squared_error(y_test, y_test_pred_sk)\n",
    "sk_test_r2   = r2_score(y_test, y_test_pred_sk)\n",
    "\n",
    "print(\"\\n=== sklearn LinearRegression (Problem 2 package) ===\")\n",
    "print(\"Training MSE:\", sk_train_mse)\n",
    "print(\"Training R^2:\", sk_train_r2)\n",
    "print(\"Testing  MSE:\", sk_test_mse)\n",
    "print(\"Testing  R^2:\", sk_test_r2)\n",
    "\n",
    "# Optional: show they match (numerical precision)\n",
    "print(\"\\nMax abs coef difference:\", np.max(np.abs(theta[1:] - sk.coef_)))\n",
    "print(\"Abs intercept difference:\", abs(theta[0] - sk.intercept_))\n",
    "\n",
    "# Example of predict_one usage:\n",
    "# pick the first test point\n",
    "example_pred = predict_one(Xte[0], theta)\n",
    "print(\"\\nExample predict_one on first test sample:\", example_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d49344e-596c-424e-983e-6fca08a82b9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
